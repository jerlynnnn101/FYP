# RAG System Tools and IBM Cloud Connectors
import os
from docling.document_converter import DocumentConverter
from langchain_docling import DoclingLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter # FIX 1 (Package install: langchain-text-splitters)
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_classic.chains import RetrievalQA # FIX 2 (Package install: langchain-classic)
from langchain_ibm import WatsonxLLM
# IBM watsonx.ai Credentials and Connection Setup
import os

os.environ["WATSONX_API_KEY"] = "heqOEuyYXy-ngbBIHva-GR8-0HNBFystyN9V6Vv2oMJB"
os.environ["IBM_PROJECT_ID"] = "0ed5949a-bd21-4f66-9733-4646506adc34"
os.environ["WATSONX_URL"] = "https://us-south.ml.cloud.ibm.com" 

print("✅ Credentials and URL loaded into session memory (Correct variable names used).")
import os
print(f"Current Working Directory: {os.getcwd()}")
# RAG Data Ingestion and Preparation Pipeline
import os
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1. Define the correct directory path
directory_path = os.path.join(os.getcwd(), "granite-snack-cookbook", "fyp_document")

print(f"Attempting to load all PDFs from directory: {directory_path}")

# 2. Load Documents using DirectoryLoader with PyPDFLoader
# PyPDFLoader is now correctly imported and used.
loader = DirectoryLoader(
    path=directory_path,
    glob="*.pdf",  # Only load files ending in .pdf
    loader_cls=PyPDFLoader  
)

# This should execute and load the files quickly.
all_documents = loader.load()

print(f"✅ Loaded {len(all_documents)} total pages/documents across all files.")

# 3. Split Text into Chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=100 
)
texts = text_splitter.split_documents(all_documents)
print(f"✅ Split documents into {len(texts)} chunks of size 1000.")
# Vector Indexing and Storage
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS 

# 1. Create Embeddings
# This uses a pre-trained model to convert text chunks into numerical vectors.
embeddings = HuggingFaceEmbeddings()

# 2. Create Vector Store (FAISS Index)
# This stores the vectors for fast retrieval.
db = FAISS.from_documents(texts, embeddings)
print("✅ Vector store created successfully using FAISS.")

# OPTIONAL: Save the index for faster reloading later
db.save_local("faiss_index_who_report")
# The Highly Optimized RAG Application Setup
import os 
from langchain_ibm import WatsonxLLM 
from langchain_classic.chains import RetrievalQA

# Corrected Imports for RAG components
from langchain_community.document_loaders import DirectoryLoader, PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# --- 1. LLM Initialization (Reliability & Output Length) ---
watsonx_llm = WatsonxLLM(                  
    model_id="ibm/granite-3-8b-instruct",                   
    project_id=os.getenv("IBM_PROJECT_ID"),
    # FIX: Use 'params' dictionary for generation controls (required by your LangChain-IBM version)
    params={'max_new_tokens': 1024, 'temperature': 0.1} # Lower temperature for factual accuracy
)

# --- 2. Document Processing and Vector Store Creation (Accuracy Tuning) ---
loader = DirectoryLoader(
    path='./granite-snack-cookbook/fyp_document', 
    loader_cls=PDFPlumberLoader, # ⬅️ USE PDFPlumberLoader HERE
    glob='*.pdf'             
)
documents = loader.load()

# OPTIMIZATION: Adjusted Chunking for better metadata retrieval (like ISBN/DOI)
text_splitter = RecursiveCharacterTextSplitter( 
    chunk_size=300,     # Very small chunk size
    chunk_overlap=150   # Greatly increased overlap to capture context across breaks
)
docs = text_splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(
    model_name="all-MiniLM-L6-v2" # Explicitly named embedding model
)

db = FAISS.from_documents(docs, embeddings) 

# --- 3. Create the RetrievalQA Chain (Citation & Speed) ---
qa_chain = RetrievalQA.from_chain_type(
    llm=watsonx_llm,           
    chain_type="stuff",      
    # OPTIMIZATION: Set k=3 for faster response time and less noise
    retriever=db.as_retriever(search_kwargs={'k': 10}), 
    # CRITICAL FOR CITATION: Returns the document chunks that were used by the LLM
    return_source_documents=True 
)     

print("✅ OPTIMIZED RAG Setup Complete.")
# 1. Manually retrieve the top 5 most similar documents
question = "What is the Sustainable Development Goal (SDG) target for the global maternal mortality ratio by 2030?"

retrieved_docs = db.similarity_search(question, k=5)

print(f"✅ Retrieved {len(retrieved_docs)} documents.")
print("\n--- CONTENT OF RETRIEVED CHUNKS ---")

# 2. Print the content and page number of each retrieved document
for i, doc in enumerate(retrieved_docs):
    # This prints the first 200 characters of the chunk
    content_snippet = doc.page_content[:200].replace('\n', ' ') 
    page_number = doc.metadata.get('page', 'N/A')
    
    # CRITICAL CHECK: Look for the ISBN, DOI, or 'PAPERBACK' in the output below.
    print(f"\n--- Document {i+1} (Page {page_number}) ---")
    print(f"Snippet: {content_snippet}...")
question = "What is the Sustainable Development Goal (SDG) target for the global maternal mortality ratio by 2030?"

# ⬇️ FIX: Use .invoke() instead of .run() ⬇️
result_dict = qa_chain.invoke({'query': question})

# --- Extract the Answer ---
answer = result_dict['result']

# --- Extract the Source Documents for Citation ---
sources = result_dict['source_documents']

print(f"Question: {question}")
print("---")
print(f"Answer: {answer}")

# --- CITATION SECTION (meets your criteria) ---
print("\n--- Sources Used (for Citation) ---")
for doc in sources:
    # This extracts the source file name and page number from the document metadata
    source_file = doc.metadata.get('source', 'N/A').split('/')[-1]
    page_number = doc.metadata.get('page', 'N/A')
    print(f"File: {source_file} (Page: {page_number})")
